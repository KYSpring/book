# 月之暗面杨植麟复盘大模型创业这一年：向延绵而未知的雪山前进

- 月之暗面（Moonshot AI）成立于2023年4月，完成新一轮超10亿美元融资，成立不到1年估值25亿美元，投资方包括红杉中国、小红书、美团、阿里。

- 大模型公司的竞争，与其说是一场科学竞争，不如说首先是一场残酷的金钱角力。在资本方捂紧口袋的情况下，你要领先对手找到更多的钱，购买更多的卡，抢夺更多的人才。

- 过去一年，国产大模型公司似乎处在一种紧迫而逼仄的生存边缘。看上去，他们每个都手握重金。但一方面，他们要把刚融的钱，立马投入极高昂的科研中追赶OpenAI——先是追齐GPT-3.5，没等追上GPT-4，Sora又来了；另一方面，他们要马不停蹄在落地场景上找可能，自我验证你是一家公司、而不是只会吞噬资本金的研究所；这还不够，每个项目不管是上市还是并购，出路更是毫不明朗。

- 杨植麟倾向于将他的公司看作是，构建一个结合科学、工程和商业的系统。你可以想象成，他要在人类世界上空，架起一张AI实验台，一手做实验，一手将尖端技术落进真实世界，通过与人类互动找到应用机会，再将应用送入消费者手中。

- 我们2023年2月开始集中做第一轮融资。如果delay（延迟）到4月，基本没机会了。但如果2022年12月或2023年1月做也没机会，当时有疫情，大家没反应过来——所以，真正窗口就是一个月。

- 你发现它是根本问题，因为你在对这个世界概率建模。虽然语言局限，它是世界的投映；但理论上你把token space（所有可能的标记组成的空间）做得更大，就可以构建一个通用世界模型。世界上每样东西怎么产生、发展，都能给它分配一个概率。所有问题都可以被归结成怎么对概率估计。

- Scaling law为什么能成为第一性原理？你只要能找到一个结构，满足两个条件：一是足够通用，二是可规模化。通用是你把所有问题放到这个框架建模，可规模化是只要你投入足够多算力，它就能变好。

- 这是我在Google学到的思维：如果能被更底层的东西解释，就不应该在上层过度雕花。

- Lab是历史了。以前Google Brain是产业界最大AI lab，但它是把研究型组织安插在大公司。这种组织能探索新想法，很难产生伟大系统——能产生Transformer，但产生不了ChatGPT。

- 我们应该学习OpenAI的技术理想主义。如果所有人都觉得你正常，你的理想是大家都能想到的，它对人类的理想总量没有增量。

- 当然马拉松刚开始，接下来会有更多差异化，这需要你提前预判到底什么是“成立的非共识”。

- 老的计算机内存，在过去几十年涨了好几个数量级，一样的事会发生在新的计算机上。它能解决很多现在的问题。比如，现在多模态架构还需要tokenizer（标记器），但当你有一个无损压缩的long context就不需要了，可以把原始的放进去。进一步讲，它是把新计算范式变成更通用的基础。

- 接下来会有两个大的milestone（里程碑）。一是真正的统一的世界模型，就是它能统一各种不同模态，一个真正的scalable和general的architecture（可扩展、通用的系统结构）。二是能在没有人类数据输入的情况下，使AI持续进化。

- 今天是把产品做得更好，有更多升维（即新的维度）。举个例子，不应该只去卷一个搜索场景，搜索在后面只是这个产品有价值的很小一部分，这个产品应该有更大增量。比传统搜索引擎好个10%、20%，没什么太大价值——只有一个颠覆性的东西，才配得上AGI这三个字。

- 开源的开发方式跟以前不一样了，以前是所有人都可以contribute（贡献）到开源，现在开源本身还是中心化的。开源的贡献可能很多都没有经过算力验证。闭源会有人才聚集和资本聚集，最后一定是闭源更好，是一个consolidation（对市场的整合）。

- 我现在觉得，你通过对视频的边际概率去建模，本质是在做无损压缩，跟语言模型next token predictions没有本质区别。只要你压缩得足够好，就可以把这个世界可以被解释的东西去进行解释。

- AI唯一work就是next token prediction + scaling law，只要token足够完整，都是可以做的。

- 我接受有失败的概率。这个事情它已经完全改变了我的生命，我是充满感激的。